{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Survival Experiments\n",
    "\n",
    "This notebook contains all our code for survival modeling.\n",
    "\n",
    "The experiments test multimodal fusion of survival models and varying dimensionality reductions for high-dimensional embeddings.\n",
    "\n",
    "Specifically, we experiment with 5 modalities:\n",
    "* Patient demographics (sex, age - binned, race, ethnicity)\n",
    "* Cancer type (we use the TCGA project ID as a proxy for cancer type)\n",
    "* RNA-seq gene expression (`BulkRNABert` embeddings)\n",
    "* Whole slide histology images (`UNI2` embeddings)\n",
    "* Pathology reports (`BioMistral` embeddings)\n",
    "\n",
    "We additionally experiment with various alternate embeddings, including:\n",
    "* `BioMistral` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology reports\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `UCE` embeddings of RNA-seq gene expression\n",
    "\n",
    "To use these alternate embeddings, modify the variables for input/output files in the first code cell of this notebook.\n",
    "\n",
    "Run experiments by executing all cells of this notebook. Results are saved in the `results` subdirectory at the root of the repo. Analysis and visualization is done using tools also in the `results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "\n",
    "expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "hist_file = \"../embed/hist.h5\" # UNI2\n",
    "text_file = \"../embed/text.h5\" # BioMistral\n",
    "output_results = \"../results/results.csv\"\n",
    "output_predictions = \"../results/predictions.npy\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "# hist_file = \"../embed/hist.h5\" # UNI2\n",
    "# text_file = \"../embed/summ.h5\" # BioMistral - Summarized\n",
    "# output_results = \"../results/results_summarized.csv\"\n",
    "# output_predictions = \"../results/predictions_summarized.npy\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# expr_file = \"../embed/expr-uce.h5\" # UCE\n",
    "# hist_file = \"../embed/hist.h5\" # UNI2\n",
    "# text_file = \"../embed/summ.h5\" # BioMistral - Summarized\n",
    "# output_results = \"../results/results_uce_summarized.csv\"\n",
    "# output_predictions = \"../results/predictions_uce_summarized.npy\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "# hist_file = \"../embed/hist.h5\" # UNI2\n",
    "# text_file = \"../embed/text-mistral.h5\" # Mistral\n",
    "# output_results = \"../results/results_mistral.csv\"\n",
    "# output_predictions = \"../results/predictions_mistral.npy\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "# hist_file = \"../embed/hist.h5\" # UNI2\n",
    "# text_file = \"../embed/summ-mistral.h5\" # Mistral - Summarized\n",
    "# output_results = \"../results/results_mistral_summarized.csv\"\n",
    "# output_predictions = \"../results/predictions_mistral_summarized.npy\"\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "# hist_file = \"../embed/hist.h5\" # UNI2\n",
    "# text_file = \"../embed/summ-corrected.h5\" # BioMistral - Summarized, Subset of manually corrected summaries\n",
    "# output_results = \"../results/results_summarized_corrected.csv\"\n",
    "# output_predictions = \"../results/predictions_summarized_corrected.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clinical.csv\")\n",
    "clin_case_ids = set(df[\"case_id\"])\n",
    "\n",
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_case_ids = set(expr_h5.keys())\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_case_ids = set(hist_h5.keys())\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_case_ids = set(text_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = sorted(list(clin_case_ids & expr_case_ids & hist_case_ids & text_case_ids))\n",
    "\n",
    "df = df[df[\"case_id\"].isin(case_ids)]\n",
    "df = df.sort_values(\"case_id\").reset_index(drop=True)\n",
    "assert df[\"case_id\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_binned\"] = pd.cut(\n",
    "    df[\"age\"],\n",
    "    bins=[0, 20, 40, 60, 80, 100],\n",
    "    labels=[\"(0, 20]\", \"(20, 40]\", \"(40, 60]\", \"(60, 80]\", \"(80, 100]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = df[\"vital_status\"] == \"Dead\"\n",
    "days_to_event = np.where(dead, df[\"days_to_death\"], df[\"days_to_last_follow_up\"])\n",
    "assert not np.isnan(days_to_event).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(zip(dead, days_to_event)), dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)\n",
    "canc_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X = demo_ohe.fit_transform(df[[\"sex\", \"age_binned\", \"race\", \"ethnicity\"]])\n",
    "canc_X = canc_ohe.fit_transform(df[[\"project\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_emb_from_h5(case_ids: list[str], h5: h5py.File):\n",
    "    X = []\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case_group = h5[case_id]\n",
    "        embs = np.stack([v[:] for v in case_group.values()], axis=0)\n",
    "        emb = np.mean(embs, axis=0)\n",
    "        X.append(emb)\n",
    "    return np.stack(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_X = extract_case_emb_from_h5(case_ids, expr_h5)\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_X = extract_case_emb_from_h5(case_ids, hist_h5)\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_X = extract_case_emb_from_h5(case_ids, text_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splitter = (\n",
    "    df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"project\"]\n",
    "    + \"_\"\n",
    "    + df[\"sex\"]\n",
    "    + \"_\"\n",
    "    + df[\"age_binned\"].astype(str)\n",
    "    + \"_\"\n",
    "    + df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"race\"]\n",
    "    + \"_\"\n",
    "    + df[\"ethnicity\"]\n",
    ")\n",
    "\n",
    "n = len(df)\n",
    "test_splits = [split_idxs for _, split_idxs in skf.split(X=np.zeros(n), y=splitter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = df[[\"case_id\"]].copy()\n",
    "meta_df[\"split\"] = -1\n",
    "meta_df[\"split_order\"] = -1\n",
    "for i, test_idxs in enumerate(test_splits):\n",
    "    meta_df.loc[test_idxs, \"split\"] = i\n",
    "    meta_df.loc[test_idxs, \"split_order\"] = list(range(len(test_idxs)))\n",
    "meta_df[\"dead\"] = y[\"Status\"]\n",
    "meta_df[\"days_to_death_or_censor\"] = y[\"Survival_in_days\"]\n",
    "meta_df.to_csv(\"../results/split_cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(\n",
    "    *,  # enforce kwargs\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Running {name}\")\n",
    "\n",
    "    # z-score input features\n",
    "    if standardize:\n",
    "        if verbose:\n",
    "            print(\"--standardized\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # dimensionality reduction\n",
    "    if pca_components is not None:\n",
    "        if verbose:\n",
    "            print(\"--reduced\")\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        X_train_red = pca.fit_transform(X_train_scaled)\n",
    "        X_test_red = pca.transform(X_test_scaled)\n",
    "    else:\n",
    "        X_train_red = X_train_scaled\n",
    "        X_test_red = X_test_scaled\n",
    "\n",
    "    # fit survival model\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.1).fit(X_train_red, y_train)\n",
    "\n",
    "    # generate predictions\n",
    "    y_train_pred = cox.predict(X_train_red)\n",
    "    y_test_pred = cox.predict(X_test_red)\n",
    "\n",
    "    # evaluate predictions\n",
    "    c_index = concordance_index_censored(\n",
    "        event_indicator=y_test[\"Status\"],\n",
    "        event_time=y_test[\"Survival_in_days\"],\n",
    "        estimate=y_test_pred,\n",
    "    )[0]\n",
    "\n",
    "    return {\n",
    "        \"c_index\": c_index,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "    }\n",
    "\n",
    "def run_unimodal_split(\n",
    "    *,  # enforce kwargs\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_idxs: np.ndarray,\n",
    "    train_idxs: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    # split matrices\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "    return run_split(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        pca_components=pca_components,\n",
    "        standardize=standardize,\n",
    "        name=name,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pca_components: int) -> dict:\n",
    "    results = []\n",
    "    for test_idxs in tqdm(test_splits, desc=\"Cross Validation Splits\"):\n",
    "        split_results = dict()\n",
    "\n",
    "        temp = set(test_idxs)\n",
    "        train_idxs = [i for i in range(n) if i not in temp]\n",
    "\n",
    "        split_results[\"demo\"] = run_unimodal_split(X=demo_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"canc\"] = run_unimodal_split(X=canc_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"expr\"] = run_unimodal_split(X=expr_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"hist\"] = run_unimodal_split(X=hist_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"text\"] = run_unimodal_split(X=text_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "\n",
    "        y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "        combos = [sorted(x) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 1]\n",
    "        for combo in combos:\n",
    "            mult_X_train = []\n",
    "            mult_X_test = []\n",
    "            for modality in combo:\n",
    "                x_train = split_results[modality][\"y_train_pred\"][:, np.newaxis]\n",
    "                x_test = split_results[modality][\"y_test_pred\"][:, np.newaxis]\n",
    "                if modality not in [\"demo\", \"canc\"]:\n",
    "                    scaler = StandardScaler()\n",
    "                    x_train = scaler.fit_transform(x_train)\n",
    "                    x_test = scaler.transform(x_test)\n",
    "                mult_X_train.append(x_train)\n",
    "                mult_X_test.append(x_test)\n",
    "\n",
    "            mult_X_train = np.concat(mult_X_train, axis=1)\n",
    "            mult_X_test = np.concat(mult_X_test, axis=1)\n",
    "\n",
    "            split_results[\"-\".join(combo)] = run_split(X_train=mult_X_train, y_train=y_train, X_test=mult_X_test, y_test=y_test, pca_components=None, standardize=False)\n",
    "\n",
    "        results.append(split_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "    results[pca_components] = run_experiment(pca_components=pca_components)\n",
    "np.save(output_predictions, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"demo\": \"demo*\",\n",
    "    \"canc\": \"canc*\",\n",
    "    \"canc-demo\": \"canc-demo*\",\n",
    "}\n",
    "combos = [\"-\".join(sorted(x)) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 0]\n",
    "df = defaultdict(dict)\n",
    "for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "    for combo in combos:\n",
    "        c_idxs = []\n",
    "        for i in range(5):\n",
    "            c_idx = results[pca_components][i][combo][\"c_index\"]\n",
    "            c_idxs.append(c_idx)\n",
    "        c_idx = np.mean(c_idxs)\n",
    "        if combo in mapping:\n",
    "            combo = mapping[combo]\n",
    "            if pca_components != 4:\n",
    "                continue\n",
    "        df[combo][pca_components] = c_idx\n",
    "df = pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "sorted_keys = sorted(sorted(combos), key=lambda x: len(x))\n",
    "sorted_keys = [x if x not in mapping else mapping[x] for x in sorted_keys]\n",
    "df = df.loc[sorted_keys]\n",
    "df.columns.name = \"pca components\"\n",
    "df.to_csv(output_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
