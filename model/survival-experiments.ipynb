{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Survival Experiments\n",
    "\n",
    "This notebook contains all our code for survival modeling. It does not include the result visualizations.\n",
    "\n",
    "The experiments test multimodal fusion of survival models and varying dimensionality reductions for high-dimensional embeddings.\n",
    "\n",
    "Specifically, we experiment with 5 modalities:\n",
    "* Patient demographics (sex, age - binned, race, ethnicity)\n",
    "* Cancer type (we use the TCGA project ID as a proxy for cancer type)\n",
    "* RNA-seq gene expression (`BulkRNABert` embeddings)\n",
    "* Whole slide histology images (`UNI2` embeddings)\n",
    "* Pathology reports (`BioMistral` embeddings)\n",
    "\n",
    "We additionally experiment with using summarizations of the pathology reports. These are summarized by `Llama-3.1-8B-Instruct` and embedding with `BioMistral`. We provide a convenience toggle at the top of this notebook to run experiments with the summarized reports: `use_summarized`.\n",
    "\n",
    "Run experiments by executing all cells of this notebook. Results are saved in the `results` subdirectory at the root of the repo. Analysis and visualization is done using tools also in the `results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_summarized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clinical.csv\")\n",
    "clin_case_ids = set(df[\"case_id\"])\n",
    "\n",
    "with h5py.File(\"../embed/expr.h5\", \"r\") as expr_h5:\n",
    "    expr_case_ids = set(expr_h5.keys())\n",
    "\n",
    "with h5py.File(\"../embed/hist.h5\", \"r\") as hist_h5:\n",
    "    hist_case_ids = set(hist_h5.keys())\n",
    "\n",
    "if not use_summarized:\n",
    "    text_file = \"../embed/text.h5\"\n",
    "else:\n",
    "    text_file = \"../embed/summ.h5\"\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_case_ids = set(text_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = sorted(list(clin_case_ids & expr_case_ids & hist_case_ids & text_case_ids))\n",
    "\n",
    "df = df[df[\"case_id\"].isin(case_ids)]\n",
    "df = df.sort_values(\"case_id\").reset_index(drop=True)\n",
    "assert df[\"case_id\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_binned\"] = pd.cut(\n",
    "    df[\"age\"],\n",
    "    bins=[0, 20, 40, 60, 80, 100],\n",
    "    labels=[\"(0, 20]\", \"(20, 40]\", \"(40, 60]\", \"(60, 80]\", \"(80, 100]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = df[\"vital_status\"] == \"Dead\"\n",
    "days_to_event = np.where(dead, df[\"days_to_death\"], df[\"days_to_last_follow_up\"])\n",
    "assert not np.isnan(days_to_event).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(zip(dead, days_to_event)), dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)\n",
    "canc_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X = demo_ohe.fit_transform(df[[\"sex\", \"age_binned\", \"race\", \"ethnicity\"]])\n",
    "canc_X = canc_ohe.fit_transform(df[[\"project\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_emb_from_h5(case_ids: list[str], h5: h5py.File):\n",
    "    X = []\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case_group = h5[case_id]\n",
    "        embs = np.stack([v[:] for v in case_group.values()], axis=0)\n",
    "        emb = np.mean(embs, axis=0)\n",
    "        X.append(emb)\n",
    "    return np.stack(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../embed/expr.h5\", \"r\") as expr_h5:\n",
    "    expr_X = extract_case_emb_from_h5(case_ids, expr_h5)\n",
    "\n",
    "with h5py.File(\"../embed/hist.h5\", \"r\") as hist_h5:\n",
    "    hist_X = extract_case_emb_from_h5(case_ids, hist_h5)\n",
    "\n",
    "with h5py.File(\"../embed/text.h5\", \"r\") as text_h5:\n",
    "    text_X = extract_case_emb_from_h5(case_ids, text_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splitter = (\n",
    "    df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"project\"]\n",
    "    + \"_\"\n",
    "    + df[\"sex\"]\n",
    "    + \"_\"\n",
    "    + df[\"age_binned\"].astype(str)\n",
    "    + \"_\"\n",
    "    + df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"race\"]\n",
    "    + \"_\"\n",
    "    + df[\"ethnicity\"]\n",
    ")\n",
    "\n",
    "n = len(df)\n",
    "test_splits = [split_idxs for _, split_idxs in skf.split(X=np.zeros(n), y=splitter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(\n",
    "    *,  # enforce kwargs\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Running {name}\")\n",
    "\n",
    "    # z-score input features\n",
    "    if standardize:\n",
    "        if verbose:\n",
    "            print(\"--standardized\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # dimensionality reduction\n",
    "    if pca_components is not None:\n",
    "        if verbose:\n",
    "            print(\"--reduced\")\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        X_train_red = pca.fit_transform(X_train_scaled)\n",
    "        X_test_red = pca.transform(X_test_scaled)\n",
    "    else:\n",
    "        X_train_red = X_train_scaled\n",
    "        X_test_red = X_test_scaled\n",
    "\n",
    "    # fit survival model\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.1).fit(X_train_red, y_train)\n",
    "\n",
    "    # generate predictions\n",
    "    y_train_pred = cox.predict(X_train_red)\n",
    "    y_test_pred = cox.predict(X_test_red)\n",
    "\n",
    "    # evaluate predictions\n",
    "    c_index = concordance_index_censored(\n",
    "        event_indicator=y_test[\"Status\"],\n",
    "        event_time=y_test[\"Survival_in_days\"],\n",
    "        estimate=y_test_pred,\n",
    "    )[0]\n",
    "\n",
    "    return {\n",
    "        \"c_index\": c_index,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "    }\n",
    "\n",
    "def run_unimodal_split(\n",
    "    *,  # enforce kwargs\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_idxs: np.ndarray,\n",
    "    train_idxs: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    # split matrices\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "    return run_split(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        pca_components=pca_components,\n",
    "        standardize=standardize,\n",
    "        name=name,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pca_components: int) -> dict:\n",
    "    results = []\n",
    "    for test_idxs in tqdm(test_splits, desc=\"Cross Validation Splits\"):\n",
    "        split_results = dict()\n",
    "\n",
    "        temp = set(test_idxs)\n",
    "        train_idxs = [i for i in range(n) if i not in temp]\n",
    "\n",
    "        split_results[\"demo\"] = run_unimodal_split(X=demo_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"canc\"] = run_unimodal_split(X=canc_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"expr\"] = run_unimodal_split(X=expr_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"hist\"] = run_unimodal_split(X=hist_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "        split_results[\"text\"] = run_unimodal_split(X=text_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_components, standardize=True)\n",
    "\n",
    "        y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "        combos = [sorted(x) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 1]\n",
    "        for combo in combos:\n",
    "            mult_X_train = []\n",
    "            mult_X_test = []\n",
    "            for modality in combo:\n",
    "                x_train = split_results[modality][\"y_train_pred\"][:, np.newaxis]\n",
    "                x_test = split_results[modality][\"y_test_pred\"][:, np.newaxis]\n",
    "                if modality not in [\"demo\", \"canc\"]:\n",
    "                    scaler = StandardScaler()\n",
    "                    x_train = scaler.fit_transform(x_train)\n",
    "                    x_test = scaler.transform(x_test)\n",
    "                mult_X_train.append(x_train)\n",
    "                mult_X_test.append(x_test)\n",
    "\n",
    "            mult_X_train = np.concat(mult_X_train, axis=1)\n",
    "            mult_X_test = np.concat(mult_X_test, axis=1)\n",
    "\n",
    "            split_results[\"-\".join(combo)] = run_split(X_train=mult_X_train, y_train=y_train, X_test=mult_X_test, y_test=y_test, pca_components=None, standardize=False)\n",
    "\n",
    "        results.append(split_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "    results[pca_components] = run_experiment(pca_components=pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_summarized:\n",
    "    np.save(\"../results/predictions.npy\", results)\n",
    "else:\n",
    "    np.save(\"../results/predictions_summarized.npy\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [\"-\".join(sorted(x)) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 0]\n",
    "df = defaultdict(dict)\n",
    "for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "    for combo in combos:\n",
    "        c_idxs = []\n",
    "        for i in range(5):\n",
    "            c_idx = results[pca_components][i][combo][\"c_index\"]\n",
    "            c_idxs.append(c_idx)\n",
    "        c_idx = np.mean(c_idxs)\n",
    "        if combo in [\"demo\", \"canc\"]:\n",
    "            combo += \"*\"\n",
    "            if pca_components != 4:\n",
    "                continue\n",
    "        df[combo][pca_components] = c_idx\n",
    "df = pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "sorted_keys = sorted(sorted(combos), key=lambda x: len(x))\n",
    "sorted_keys = [\"canc*\", \"demo*\"] + sorted_keys[2:]\n",
    "df = df.loc[sorted_keys]\n",
    "df.columns.name = \"pca components\"\n",
    "if not use_summarized:\n",
    "    df.to_csv(\"../results/results.csv\")\n",
    "else:\n",
    "    df.index = df.index.str.replace(\"text\", \"summ\")\n",
    "    df.to_csv(\"../results/results_summarized.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
